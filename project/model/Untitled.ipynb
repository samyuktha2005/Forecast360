{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e129071-8e98-4a92-96d8-d075b261491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Historical Weather Data Fetched Successfully!\n",
      "\n",
      "üìä Hourly Weather Data Sample:\n",
      "                 time  temperature_2m  relative_humidity_2m  dew_point_2m  \\\n",
      "0 2024-12-06 00:00:00             2.3                    79          -1.0   \n",
      "1 2024-12-06 01:00:00             3.2                    73          -1.2   \n",
      "2 2024-12-06 02:00:00             3.9                    65          -2.1   \n",
      "3 2024-12-06 03:00:00             4.1                    67          -1.5   \n",
      "4 2024-12-06 04:00:00             3.6                    79           0.3   \n",
      "\n",
      "   apparent_temperature  precipitation_probability  precipitation  rain  \\\n",
      "0                  -3.2                         15            0.0   0.0   \n",
      "1                  -2.1                         23            0.0   0.0   \n",
      "2                  -1.6                         29            0.0   0.0   \n",
      "3                  -1.5                         35            0.0   0.0   \n",
      "4                  -1.8                         45            0.6   0.6   \n",
      "\n",
      "   showers  snowfall  ...  evapotranspiration  et0_fao_evapotranspiration  \\\n",
      "0      0.0       0.0  ...                 0.0                        0.02   \n",
      "1      0.0       0.0  ...                 0.0                        0.03   \n",
      "2      0.0       0.0  ...                 0.0                        0.04   \n",
      "3      0.0       0.0  ...                 0.0                        0.04   \n",
      "4      0.0       0.0  ...                 0.0                        0.02   \n",
      "\n",
      "   vapour_pressure_deficit  wind_speed_180m  wind_direction_180m  \\\n",
      "0                     0.15              NaN                  NaN   \n",
      "1                     0.21              NaN                  NaN   \n",
      "2                     0.28              NaN                  NaN   \n",
      "3                     0.27              NaN                  NaN   \n",
      "4                     0.17              NaN                  NaN   \n",
      "\n",
      "   wind_gusts_10m  temperature_180m  soil_temperature_54cm  \\\n",
      "0            49.7               NaN                    NaN   \n",
      "1            47.2               NaN                    NaN   \n",
      "2            44.3               NaN                    NaN   \n",
      "3            44.3               NaN                    NaN   \n",
      "4            47.5               NaN                    NaN   \n",
      "\n",
      "   soil_moisture_27_to_81cm  is_day  \n",
      "0                       NaN       0  \n",
      "1                       NaN       0  \n",
      "2                       NaN       0  \n",
      "3                       NaN       0  \n",
      "4                       NaN       0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "üìä Daily Weather Data Sample:\n",
      "        time  weather_code  temperature_2m_max  temperature_2m_min  \\\n",
      "0 2024-12-06            63                 6.7                 2.3   \n",
      "1 2024-12-07            53                 6.1                 4.9   \n",
      "2 2024-12-08            51                 5.3                 3.3   \n",
      "3 2024-12-09            51                 4.9                 2.8   \n",
      "4 2024-12-10             3                 3.3                 1.1   \n",
      "\n",
      "   apparent_temperature_max  apparent_temperature_min           sunrise  \\\n",
      "0                       4.1                      -3.2  2024-12-06T08:01   \n",
      "1                       3.2                       0.8  2024-12-07T08:03   \n",
      "2                       2.3                      -0.8  2024-12-08T08:04   \n",
      "3                       0.9                      -1.2  2024-12-09T08:05   \n",
      "4                      -0.6                      -2.5  2024-12-10T08:06   \n",
      "\n",
      "             sunset  daylight_duration  sunshine_duration  ...  rain_sum  \\\n",
      "0  2024-12-06T15:53           28292.23               0.00  ...      10.0   \n",
      "1  2024-12-07T15:52           28196.63           20194.23  ...       2.5   \n",
      "2  2024-12-08T15:52           28106.73              60.77  ...       0.3   \n",
      "3  2024-12-09T15:52           28022.74               0.00  ...       1.7   \n",
      "4  2024-12-10T15:52           27944.84           20324.47  ...       0.0   \n",
      "\n",
      "   showers_sum  snowfall_sum  precipitation_hours  \\\n",
      "0          0.0           0.0                  9.0   \n",
      "1          0.0           0.0                  8.0   \n",
      "2          0.0           0.0                  3.0   \n",
      "3          0.0           0.0                  9.0   \n",
      "4          0.0           0.0                  0.0   \n",
      "\n",
      "   precipitation_probability_max  wind_speed_10m_max  wind_gusts_10m_max  \\\n",
      "0                            100                24.0                50.0   \n",
      "1                             81                17.9                36.7   \n",
      "2                             47                16.2                31.7   \n",
      "3                             71                20.9                41.4   \n",
      "4                              0                19.1                42.1   \n",
      "\n",
      "   wind_direction_10m_dominant  shortwave_radiation_sum  \\\n",
      "0                          180                     0.28   \n",
      "1                          181                     1.96   \n",
      "2                           78                     0.37   \n",
      "3                           47                     0.13   \n",
      "4                           59                     1.86   \n",
      "\n",
      "   et0_fao_evapotranspiration  \n",
      "0                        0.35  \n",
      "1                        0.54  \n",
      "2                        0.27  \n",
      "3                        0.27  \n",
      "4                        0.34  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Open-Meteo API Endpoint\n",
    "BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Location: Berlin, Germany (Example)\n",
    "LAT, LON = 52.52, 13.41  # Change to your desired location\n",
    "\n",
    "# Number of past days to fetch (Max = 92 days)\n",
    "PAST_DAYS = 92  \n",
    "\n",
    "# API Parameters with All Features\n",
    "params = {\n",
    "    \"latitude\": LAT,\n",
    "    \"longitude\": LON,\n",
    "    \"past_days\": PAST_DAYS,\n",
    "    \"hourly\": \"temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,\"\n",
    "              \"precipitation_probability,precipitation,rain,showers,snowfall,snow_depth,\"\n",
    "              \"weather_code,pressure_msl,surface_pressure,cloud_cover,cloud_cover_low,\"\n",
    "              \"cloud_cover_mid,cloud_cover_high,visibility,evapotranspiration,\"\n",
    "              \"et0_fao_evapotranspiration,vapour_pressure_deficit,wind_speed_180m,\"\n",
    "              \"wind_direction_180m,wind_gusts_10m,temperature_180m,soil_temperature_54cm,\"\n",
    "              \"soil_moisture_27_to_81cm,is_day\",\n",
    "    \"daily\": \"weather_code,temperature_2m_max,temperature_2m_min,apparent_temperature_max,\"\n",
    "             \"apparent_temperature_min,sunrise,sunset,daylight_duration,sunshine_duration,\"\n",
    "             \"uv_index_max,precipitation_sum,rain_sum,showers_sum,snowfall_sum,\"\n",
    "             \"precipitation_hours,precipitation_probability_max,wind_speed_10m_max,\"\n",
    "             \"wind_gusts_10m_max,wind_direction_10m_dominant,shortwave_radiation_sum,\"\n",
    "             \"et0_fao_evapotranspiration\",\n",
    "    \"timezone\": \"auto\"\n",
    "}\n",
    "\n",
    "# Make API Request\n",
    "response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "# Check for successful response\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"‚úÖ Historical Weather Data Fetched Successfully!\")\n",
    "    \n",
    "    # Extract and format hourly data\n",
    "    hourly_data = pd.DataFrame(data[\"hourly\"])\n",
    "    if \"time\" in hourly_data:\n",
    "        hourly_data[\"time\"] = pd.to_datetime(hourly_data[\"time\"])  # Convert time to datetime\n",
    "    \n",
    "    print(\"\\nüìä Hourly Weather Data Sample:\")\n",
    "    print(hourly_data.head())\n",
    "\n",
    "    # Extract and format daily data\n",
    "    daily_data = pd.DataFrame(data[\"daily\"])\n",
    "    if \"time\" in daily_data:\n",
    "        daily_data[\"time\"] = pd.to_datetime(daily_data[\"time\"])  # Convert time to datetime\n",
    "    \n",
    "    print(\"\\nüìä Daily Weather Data Sample:\")\n",
    "    print(daily_data.head())\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1f27762-3316-4be9-a3c9-d44fef2ff1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2376, 29)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8052d9c0-4da4-45e6-b471-0bb8d63ec244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 22)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bfe4972-382d-45b3-b659-3e85aa0074dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data saved successfully as CSV files!\n"
     ]
    }
   ],
   "source": [
    "# Save Hourly Data\n",
    "hourly_data.to_csv(\"hourly_weather_data.csv\", index=False)\n",
    "\n",
    "# Save Daily Data\n",
    "daily_data.to_csv(\"daily_weather_data.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Data saved successfully as CSV files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbda749d-f7bf-40d2-8533-b5d39da23d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved in: C:\\Users\\samyu\\Downloads\\project-bolt-sb1-8uymgqgg\\project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"Files saved in:\", cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd507eaa-8085-4c8c-b169-e0de2891b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data.to_csv(\"C:/Users/samyu/Documents/hourly_weather_data.csv\", index=False)\n",
    "daily_data.to_csv(\"C:/Users/samyu/Documents/daily_weather_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb08489-255c-4eb3-930d-120b7298530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['time', 'weather_code', 'temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min', 'sunrise', 'sunset', 'daylight_duration', 'sunshine_duration', 'uv_index_max', 'precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'wind_speed_10m_max', 'wind_gusts_10m_max', 'wind_direction_10m_dominant', 'shortwave_radiation_sum', 'et0_fao_evapotranspiration']\n",
      "Columns after renaming: ['time', 'weather_code', 'T (¬∞C)', 'Tdew (¬∞C)', 'apparent_temperature_max', 'apparent_temperature_min', 'sunrise', 'sunset', 'daylight_duration', 'sunshine_duration', 'uv_index_max', 'precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'wv (km/h)', 'max. wv (km/h)', 'wd (¬∞)', 'shortwave_radiation_sum', 'et0_fao_evapotranspiration', 'Date Time (ISO 8601)']\n",
      "Warning: Missing columns after renaming: ['p (mbar)', 'rh (%)']\n",
      "Processed dataset saved at: C:\\Users\\samyu\\Downloads\\project-bolt-sb1-mdwuunns\\model\\standardized_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samyu\\AppData\\Local\\Temp\\ipykernel_45300\\2452236841.py:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['rh (%)'].fillna(50, inplace=True)  # Default to 50% if NaN remains\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\samyu\\Downloads\\project-bolt-sb1-mdwuunns\\model\\daily_weather_data.csv\"\n",
    "output_file = r\"C:\\Users\\samyu\\Downloads\\project-bolt-sb1-mdwuunns\\model\\standardized_weather_data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Convert 'time' column to proper datetime format if it exists\n",
    "if \"time\" in df.columns:\n",
    "    df[\"Date Time (ISO 8601)\"] = pd.to_datetime(df[\"time\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Rename columns to match required format\n",
    "rename_dict = {\n",
    "    \"temperature_2m_max\": \"T (¬∞C)\",\n",
    "    \"temperature_2m_min\": \"Tdew (¬∞C)\",  # Assuming min temp is dew point\n",
    "    \"pressure_msl\": \"p (mbar)\",  # Ensure correct pressure naming\n",
    "    \"relative_humidity_2m_max\": \"rh (%)\",\n",
    "    \"wind_speed_10m_max\": \"wv (km/h)\",\n",
    "    \"wind_gusts_10m_max\": \"max. wv (km/h)\",\n",
    "    \"wind_direction_10m_dominant\": \"wd (¬∞)\",\n",
    "}\n",
    "\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Print column names after renaming\n",
    "print(\"Columns after renaming:\", df.columns.tolist())\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = rename_dict.values()\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(\"Warning: Missing columns after renaming:\", missing_columns)\n",
    "    for col in missing_columns:\n",
    "        df[col] = np.nan  # Fill missing columns with NaN\n",
    "\n",
    "# Constants\n",
    "R_SPECIFIC = 287.05  # J/kg¬∑K (Specific gas constant for dry air)\n",
    "\n",
    "# Calculate vapor pressures\n",
    "df['VPmax (mbar)'] = 6.1078 * 10 ** ((7.5 * df['T (¬∞C)']) / (df['T (¬∞C)'] + 237.3))\n",
    "df['VPact (mbar)'] = 6.1078 * 10 ** ((7.5 * df['Tdew (¬∞C)']) / (df['Tdew (¬∞C)'] + 237.3))\n",
    "df['VPdef (mbar)'] = df['VPmax (mbar)'] - df['VPact (mbar)']\n",
    "df['rh (%)'] = 100 * (df['VPact (mbar)'] / df['VPmax (mbar)'])\n",
    "df['rh (%)'].fillna(50, inplace=True)  # Default to 50% if NaN remains\n",
    "\n",
    "# Calculate specific humidity (sh) and water content\n",
    "df['sh (g/kg)'] = (0.622 * df['VPact (mbar)']) / (df['p (mbar)'].fillna(1013.25) - (0.378 * df['VPact (mbar)'])) * 1000\n",
    "df['H2OC (mmol/mol)'] = df['sh (g/kg)'] * 1000 / 18\n",
    "\n",
    "# Convert temperature to Kelvin\n",
    "df[\"Tpot (K)\"] = df[\"T (¬∞C)\"].fillna(0) + 273.15  \n",
    "\n",
    "# Calculate air density (œÅ)\n",
    "df['rho (g/m**3)'] = df['p (mbar)'].fillna(1013.25) * 100 / (R_SPECIFIC * df['Tpot (K)'])\n",
    "\n",
    "# **NEW: Calculate virtual temperature (Tv)**\n",
    "df[\"q\"] = df[\"sh (g/kg)\"] / 1000  # Convert g/kg to kg/kg\n",
    "df[\"Tv (K)\"] = df[\"Tpot (K)\"] * (1 + 0.61 * df[\"q\"])\n",
    "\n",
    "# **NEW: Calculate pressure (p in mbar) dynamically**\n",
    "df[\"p (mbar)\"] = (df[\"rho (g/m**3)\"] * R_SPECIFIC * df[\"Tv (K)\"]) / 100\n",
    "\n",
    "# Convert wind speeds\n",
    "df[\"wv (m/s)\"] = df[\"wv (km/h)\"].fillna(0) / 3.6\n",
    "df[\"max. wv (m/s)\"] = df[\"max. wv (km/h)\"].fillna(0) / 3.6\n",
    "\n",
    "# Ensure all required columns exist\n",
    "desired_columns = [\n",
    "    \"Date Time (ISO 8601)\", \"p (mbar)\", \"T (¬∞C)\", \"Tpot (K)\", \"Tdew (¬∞C)\",\n",
    "    \"rh (%)\", \"VPmax (mbar)\", \"VPact (mbar)\", \"VPdef (mbar)\", \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\", \"rho (g/m**3)\", \"wv (m/s)\", \"max. wv (m/s)\", \"wd (¬∞)\"\n",
    "]\n",
    "\n",
    "for col in desired_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan  # Ensure all columns exist\n",
    "\n",
    "df = df[desired_columns]  # Reorder columns\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Processed dataset saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "93c70b03-9afb-41da-adff-e3356fcaeb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2376 entries, 0 to 2375\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Date Time (ISO 8601)  2376 non-null   object \n",
      " 1   p (mbar)              0 non-null      float64\n",
      " 2   T (¬∞C)                0 non-null      float64\n",
      " 3   Tpot (K)              2376 non-null   float64\n",
      " 4   Tdew (¬∞C)             0 non-null      float64\n",
      " 5   rh (%)                2376 non-null   float64\n",
      " 6   VPmax (mbar)          0 non-null      float64\n",
      " 7   VPact (mbar)          0 non-null      float64\n",
      " 8   VPdef (mbar)          0 non-null      float64\n",
      " 9   sh (g/kg)             0 non-null      float64\n",
      " 10  H2OC (mmol/mol)       0 non-null      float64\n",
      " 11  rho (g/m**3)          2376 non-null   float64\n",
      " 12  wv (m/s)              2376 non-null   float64\n",
      " 13  max. wv (m/s)         2376 non-null   float64\n",
      " 14  wd (¬∞)                0 non-null      float64\n",
      "dtypes: float64(14), object(1)\n",
      "memory usage: 278.6+ KB\n",
      "['2024-12-06 00:00:00' '2024-12-06 01:00:00' '2024-12-06 02:00:00' ...\n",
      " '2025-03-14 21:00:00' '2025-03-14 22:00:00' '2025-03-14 23:00:00']\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(df['Date Time (ISO 8601)'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49661258-629b-4b65-bf72-cb962170d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the API retrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f08d9bf0-388e-4a07-98d0-797339afcd93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Timestamp column is missing in API dataset. Add or generate it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure there's a timestamp column (modify this based on actual column names)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m api_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimestamp column is missing in API dataset. Add or generate it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert timestamp column to datetime format\u001b[39;00m\n\u001b[0;32m     11\u001b[0m api_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(api_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Timestamp column is missing in API dataset. Add or generate it."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load API-extracted dataset\n",
    "api_data = pd.read_csv(\"standardized_weather_data.csv\")\n",
    "\n",
    "# Ensure there's a timestamp column (modify this based on actual column names)\n",
    "if 'timestamp' not in api_data.columns:\n",
    "    raise ValueError(\"Timestamp column is missing in API dataset. Add or generate it.\")\n",
    "\n",
    "# Convert timestamp column to datetime format\n",
    "api_data['timestamp'] = pd.to_datetime(api_data['timestamp'])\n",
    "\n",
    "# Set timestamp as index\n",
    "api_data.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample data to 10-minute intervals (matching Jena Climate dataset)\n",
    "api_data_resampled = api_data.resample('10T').interpolate(method='linear')\n",
    "\n",
    "# Save the resampled data\n",
    "api_data_resampled.to_csv(\"resampled_weather_data.csv\")\n",
    "\n",
    "print(\"API dataset resampled to 10-minute intervals and saved as resampled_weather_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "985b8b82-e31f-4850-bba2-75d16f8436ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   A       100 non-null    int64\n",
      " 1   B       100 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 1.7 KB\n"
     ]
    }
   ],
   "source": [
    "df_shifted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c8b1139-7246-4148-957e-b8666101657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPdef (mbar)       0\n",
      "wv (m/s)           0\n",
      "p (mbar)           0\n",
      "VPact (mbar)       0\n",
      "Tpot (K)           0\n",
      "VPmax (mbar)       0\n",
      "max. wv (m/s)      0\n",
      "rho (g/m**3)       0\n",
      "H2OC (mmol/mol)    0\n",
      "date_time          0\n",
      "sh (g/kg)          0\n",
      "rh (%)             0\n",
      "dtype: int64\n",
      "(420640, 12)\n",
      "   VPdef (mbar)  wv (m/s)     p (mbar)  VPact (mbar)  Tpot (K)  VPmax (mbar)  \\\n",
      "0      2.339745  7.000000  1017.392663     10.874116    284.25     13.213861   \n",
      "1      0.999421  6.333333  1017.392663     10.874116    282.65     11.873537   \n",
      "2      4.232198  4.694444  1016.736722      9.158213    284.45     13.390411   \n",
      "3      5.199762  6.166667  1016.935828      9.679296    286.05     14.879059   \n",
      "4      1.509438  6.833333  1016.258621      7.906140    279.25      9.415579   \n",
      "\n",
      "   max. wv (m/s)  rho (g/m**3)  H2OC (mmol/mol)  date_time  sh (g/kg)  \\\n",
      "0      16.388889      1.241820       372.357933 2024-12-16   6.702443   \n",
      "1      14.111111      1.248849       372.357933 2024-12-17   6.702443   \n",
      "2      11.000000      1.240947       313.399518 2024-12-18   5.641191   \n",
      "3      13.611111      1.234006       331.295931 2024-12-19   5.963327   \n",
      "4      14.500000      1.264055       270.426094 2024-12-20   4.867670   \n",
      "\n",
      "      rh (%)  \n",
      "0  82.293254  \n",
      "1  91.582789  \n",
      "2  68.393816  \n",
      "3  65.053151  \n",
      "4  83.968713  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rename 'Date Time' columns for consistency\n",
    "df_shifted.rename(columns={\"Date Time (ISO 8601)\": \"date_time\"}, inplace=True)\n",
    "jena_climate_2009_2016.rename(columns={\"Date Time\": \"date_time\"}, inplace=True)\n",
    "\n",
    "# Convert 'date_time' to datetime format\n",
    "df_shifted[\"date_time\"] = pd.to_datetime(df_shifted[\"date_time\"], errors=\"coerce\")\n",
    "jena_climate_2009_2016[\"date_time\"] = pd.to_datetime(jena_climate_2009_2016[\"date_time\"], errors=\"coerce\")\n",
    "\n",
    "# Find common columns\n",
    "common_columns = list(set(df_shifted.columns) & set(jena_climate_2009_2016.columns))\n",
    "\n",
    "# Select only the common columns from both dataframes\n",
    "df_shifted = df_shifted[common_columns]\n",
    "jena_climate_2009_2016 = jena_climate_2009_2016[common_columns]\n",
    "\n",
    "# Concatenate row-wise\n",
    "combined_df = pd.concat([df_shifted, jena_climate_2009_2016], ignore_index=True)\n",
    "\n",
    "# Check for NaN values\n",
    "print(combined_df.isna().sum())  # Count missing values per column\n",
    "print(combined_df.shape)  # Check the final dataframe shape\n",
    "print(combined_df.head())  # Preview data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b53161a-fa81-424f-bbea-4f297833a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420640 entries, 0 to 420639\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   VPdef (mbar)     420640 non-null  float64       \n",
      " 1   wv (m/s)         420640 non-null  float64       \n",
      " 2   p (mbar)         420640 non-null  float64       \n",
      " 3   VPact (mbar)     420640 non-null  float64       \n",
      " 4   Tpot (K)         420640 non-null  float64       \n",
      " 5   VPmax (mbar)     420640 non-null  float64       \n",
      " 6   max. wv (m/s)    420640 non-null  float64       \n",
      " 7   rho (g/m**3)     420640 non-null  float64       \n",
      " 8   H2OC (mmol/mol)  420640 non-null  float64       \n",
      " 9   date_time        420640 non-null  datetime64[ns]\n",
      " 10  sh (g/kg)        420640 non-null  float64       \n",
      " 11  rh (%)           420640 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11)\n",
      "memory usage: 38.5 MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6ee1b3d-3911-468d-934e-40099ce3b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPdef (mbar)       0\n",
      "wv (m/s)           0\n",
      "p (mbar)           0\n",
      "VPact (mbar)       0\n",
      "Tpot (K)           0\n",
      "VPmax (mbar)       0\n",
      "max. wv (m/s)      0\n",
      "rho (g/m**3)       0\n",
      "H2OC (mmol/mol)    0\n",
      "date_time          0\n",
      "sh (g/kg)          0\n",
      "rh (%)             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Option 1: Fill NaNs with interpolation (recommended for time-series)\n",
    "combined_df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Option 2: Drop rows with NaNs (if very few)\n",
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68777bf0-cbca-4fdf-bd1c-bb61fc825e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VPdef (mbar)  wv (m/s)  p (mbar)  VPact (mbar)  Tpot (K)  VPmax (mbar)  \\\n",
      "0      0.050853  0.997857  1.000000      0.366296  0.554001      0.195222   \n",
      "1      0.021722  0.997790  1.000000      0.366296  0.527659      0.173886   \n",
      "2      0.091984  0.997627  0.993680      0.303967  0.557293      0.198033   \n",
      "3      0.113014  0.997774  0.995599      0.322895  0.583635      0.221730   \n",
      "4      0.032807  0.997840  0.989074      0.258487  0.471683      0.134759   \n",
      "\n",
      "   max. wv (m/s)  rho (g/m**3)  H2OC (mmol/mol)  date_time  sh (g/kg)  \\\n",
      "0       0.999290      0.000024         1.000000 2024-12-16   0.351812   \n",
      "1       0.999063      0.000029         1.000000 2024-12-17   0.351812   \n",
      "2       0.998753      0.000023         0.841321 2024-12-18   0.291616   \n",
      "3       0.999013      0.000018         0.889487 2024-12-19   0.309888   \n",
      "4       0.999102      0.000039         0.725664 2024-12-20   0.247741   \n",
      "\n",
      "     rh (%)  \n",
      "0  0.796591  \n",
      "1  0.903306  \n",
      "2  0.636919  \n",
      "3  0.598543  \n",
      "4  0.815838  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select numerical columns (excluding 'date_time')\n",
    "num_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform numerical columns\n",
    "combined_df[num_cols] = scaler.fit_transform(combined_df[num_cols])\n",
    "\n",
    "# Save scaler for future inverse transformation (if needed)\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Check normalized data\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b9eb490-6be6-4f90-a1d5-64bb8aba3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (420630, 10, 12)\n",
      "Shape of Y: (420630, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sliding_window_sequences(data, window_size):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])  # Past 'window_size' steps\n",
    "        Y.append(data[i + window_size])    # Predict next step\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Example window size (choose based on your prediction goal)\n",
    "window_size = 10  \n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "data_array = combined_df.values  # Assuming merged_df is your final stacked dataset\n",
    "\n",
    "# Create sequences\n",
    "X, Y = create_sliding_window_sequences(data_array, window_size)\n",
    "\n",
    "# Print shape to verify\n",
    "print(\"Shape of X:\", X.shape)  # (samples, window_size, features)\n",
    "print(\"Shape of Y:\", Y.shape)  # (samples, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c916737e-70e8-4a34-80a4-005998b66fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T (¬∞C)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'T (¬∞C)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m data_array \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming combined_df is your final stacked dataset\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create sequences\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sliding_window_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Split into training (80%) and testing (20%) sets\u001b[39;00m\n\u001b[0;32m     26\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# No shuffle for time-series!\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[81], line 7\u001b[0m, in \u001b[0;36mcreate_sliding_window_sequences\u001b[1;34m(data, target_column, window_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_sliding_window_sequences\u001b[39m(data, target_column, window_size):\n\u001b[0;32m      5\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m----> 7\u001b[0m     target_idx \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get column index of target\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m window_size):\n\u001b[0;32m     10\u001b[0m         X\u001b[38;5;241m.\u001b[39mappend(data[i:i \u001b[38;5;241m+\u001b[39m window_size])  \u001b[38;5;66;03m# Past 'window_size' steps\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'T (¬∞C)'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sliding_window_sequences(data, target_column, window_size):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    target_idx = combined_df.columns.get_loc(target_column)  # Get column index of target\n",
    "    \n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])  # Past 'window_size' steps\n",
    "        Y.append(data[i + window_size, target_idx])  # Predict target value at next step\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Example window size\n",
    "window_size = 10  \n",
    "target_column = \"T (¬∞C)\"  # Your target column\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "data_array = combined_df.values  # Assuming combined_df is your final stacked dataset\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sliding_window_sequences(data_array, target_column, window_size)\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  # No shuffle for time-series!\n",
    "\n",
    "# Reshape y to (samples, 1) for LSTM\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Print final shapes\n",
    "print(f\"Training Data Shape: {X_train.shape}, Labels Shape: {y_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}, Labels Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37e03126-bb70-4c7c-9641-c0f38f5978c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420640 entries, 0 to 420639\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   VPdef (mbar)     420640 non-null  float64       \n",
      " 1   wv (m/s)         420640 non-null  float64       \n",
      " 2   p (mbar)         420640 non-null  float64       \n",
      " 3   VPact (mbar)     420640 non-null  float64       \n",
      " 4   Tpot (K)         420640 non-null  float64       \n",
      " 5   VPmax (mbar)     420640 non-null  float64       \n",
      " 6   max. wv (m/s)    420640 non-null  float64       \n",
      " 7   rho (g/m**3)     420640 non-null  float64       \n",
      " 8   H2OC (mmol/mol)  420640 non-null  float64       \n",
      " 9   date_time        420640 non-null  datetime64[ns]\n",
      " 10  sh (g/kg)        420640 non-null  float64       \n",
      " 11  rh (%)           420640 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11)\n",
      "memory usage: 38.5 MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fab76-fcc3-40a6-abf5-57cbfd7975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "# Define model function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('units1', min_value=32, max_value=256, step=32), return_sequences=True, input_shape=(10, 12)))\n",
    "    model.add(LSTM(hp.Int('units2', min_value=32, max_value=128, step=32), return_sequences=False))\n",
    "    model.add(Dense(12))  # Output layer\n",
    "\n",
    "    # Tune Learning Rate\n",
    "    lr = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model, \n",
    "    objective='val_loss', \n",
    "    max_epochs=30, \n",
    "    factor=3, \n",
    "    directory='tuner_results', \n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "tuner.search(X, Y, epochs=30, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(X, Y, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = best_model.evaluate(X, Y)\n",
    "print(f\"Optimized Loss: {loss}, MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "839c2d2f-302e-4482-84ab-a46d40701413",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Kaggle dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m kaggle_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjena_climate_2009_2016.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with actual filename\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate time difference\u001b[39;00m\n\u001b[0;32m      7\u001b[0m time_diffs \u001b[38;5;241m=\u001b[39m kaggle_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[0;32m    156\u001b[0m             usecols,\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:243\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    233\u001b[0m missing_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m    235\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column provided to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_dates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    248\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[0;32m    250\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'timestamp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_df = pd.read_csv(\"jena_climate_2009_2016.csv\", parse_dates=[\"timestamp\"])  # Replace with actual filename\n",
    "\n",
    "# Calculate time difference\n",
    "time_diffs = kaggle_df[\"timestamp\"].diff().dropna()\n",
    "\n",
    "# Get the most common time step\n",
    "time_step = time_diffs.value_counts().idxmax()\n",
    "\n",
    "print(\"Detected time step:\", time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f49e3a-117b-4e62-8a1c-a73bd303551c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6ae0-ddc4-4ee6-aa18-300f36f47ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a3cdd-6337-4265-bbca-f04d0cf5e6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039329e1-e286-4764-86e6-daba164d3fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbcd90-5566-4ca6-a77a-40dda1191050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8045-e399-4e4f-8a27-05bf86bdfeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427b180-be2a-49a0-b07f-136425040e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c407499-e9b7-472f-ab8a-85e6e7ef36f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61741386-ff38-4cf1-a418-72eaf45cd628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054604b-4f13-4ac7-965d-692b9303536a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be083b-93d6-4d27-ae82-31776a93de3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a985b3-b484-45ce-9812-bf54f85346ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78d9d1-d96c-49bb-b7f1-8630a358cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\samyu\\Downloads\\project-bolt-sb1-mdwuunns\\model\\daily_weather_data.csv\"\n",
    "output_file = r\"C:\\Users\\samyu\\Downloads\\project-bolt-sb1-mdwuunns\\model\\standardized_weather_data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Convert 'time' column to proper datetime format\n",
    "df[\"Date Time (ISO 8601)\"] = pd.to_datetime(df[\"time\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Rename columns to match standard format\n",
    "rename_dict = {\n",
    "    \"temperature_2m_max\": \"T (¬∞C)\",\n",
    "    \"temperature_2m_min\": \"Tdew (¬∞C)\",  # Assuming Min Temp is Dew Point Temp\n",
    "    \"p (mbar)\": \"p (hPa)\",  # Convert naming convention\n",
    "    \"wind_speed_max\": \"wv (km/h)\",\n",
    "    \"wind_gust_max\": \"max. wv (km/h)\",\n",
    "    \"wind_direction\": \"wd (¬∞)\"\n",
    "}\n",
    "\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Ensure missing wind speed columns exist and check if they contain actual values\n",
    "if \"wind_speed_max\" in df.columns:\n",
    "    df.rename(columns={\"wind_speed_max\": \"wv (km/h)\"}, inplace=True)\n",
    "    df[\"wv (km/h)\"] = df[\"wv (km/h)\"].fillna(0)  # Replace NaN with 0\n",
    "\n",
    "if \"wind_gust_max\" in df.columns:\n",
    "    df.rename(columns={\"wind_gust_max\": \"max. wv (km/h)\"}, inplace=True)\n",
    "    df[\"max. wv (km/h)\"] = df[\"max. wv (km/h)\"].fillna(0)\n",
    "\n",
    "if \"wind_direction\" in df.columns:\n",
    "    df.rename(columns={\"wind_direction\": \"wd (¬∞)\"}, inplace=True)\n",
    "    df[\"wd (¬∞)\"] = df[\"wd (¬∞)\"].fillna(\"Unknown\")  # Keep Unknown for missing wind direction\n",
    "\n",
    "# Print to verify changes\n",
    "print(df.head())\n",
    "\n",
    "# Rename wind-related columns\n",
    "df.rename(columns={\n",
    "    \"wind_speed_10m_max\": \"wv (km/h)\",\n",
    "    \"wind_gusts_10m_max\": \"max. wv (km/h)\",\n",
    "    \"wind_direction_10m_dominant\": \"wd (¬∞)\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure no missing values in wind-related columns\n",
    "df[\"wv (km/h)\"] = df[\"wv (km/h)\"].fillna(0)  # Replace NaN with 0\n",
    "df[\"max. wv (km/h)\"] = df[\"max. wv (km/h)\"].fillna(0)\n",
    "df[\"wd (¬∞)\"] = df[\"wd (¬∞)\"].fillna(\"Unknown\")  # Replace NaN with 'Unknown'\n",
    "\n",
    "# Verify the changes\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Ensure the column exists before trying to modify it\n",
    "if \"p (hPa)\" in df.columns:\n",
    "    df[\"p (hPa)\"] = df[\"p (hPa)\"].astype(float)\n",
    "\n",
    "# Fix Tpot (K) values\n",
    "if \"T (¬∞C)\" in df.columns:\n",
    "    df[\"Tpot (K)\"] = df[\"T (¬∞C)\"] + 273.15\n",
    "else:\n",
    "    df[\"Tpot (K)\"] = None  # Handle missing values\n",
    "\n",
    "# Ensure missing wind speed columns exist\n",
    "for col in [\"wv (km/h)\", \"max. wv (km/h)\", \"wd (¬∞)\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None  # Assign default value if missing\n",
    "\n",
    "# Reorder columns to match standard format\n",
    "standard_columns = [\n",
    "    \"Date Time (ISO 8601)\", \"p (hPa)\", \"T (¬∞C)\", \"Tpot (K)\", \"Tdew (¬∞C)\",\n",
    "    \"rh (%)\", \"VPmax (hPa)\", \"VPact (hPa)\", \"VPdef (hPa)\", \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\", \"rho (g/m¬≥)\", \"wv (km/h)\", \"max. wv (km/h)\", \"wd (¬∞)\"\n",
    "]\n",
    "\n",
    "# Keep only available columns from standard list\n",
    "df = df[[col for col in standard_columns if col in df.columns]]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Converted dataset saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc87a9-e41d-449c-a1c2-7031d5d1346a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd7c22-ca07-46a2-8795-316941dc370f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
